models:
  gemini:
    provider: gemini
    model_name: gemini-2.5-pro
    context_window: 1000000
    api_key_env: GEMINI_API_KEY
    # Pricing: USD per token (not per million). Tiered by input token count.
    pricing:
      input_cost_per_token_usd: 0.0     # ignored when tiers present
      output_cost_per_token_usd: 0.0    # ignored when tiers present
      tiers:
        # Tier 1: up to 200k input tokens
        - upto_input_tokens: 200000
          input_cost_per_token_usd: 0.00000125   # $1.25 / 1M
          cached_input_cost_per_token_usd: 0.0000003125 # $0.3125 / 1M
          output_cost_per_token_usd: 0.00001     # $10 / 1M
        # Tier 2: above 200k input tokens
        - upto_input_tokens: 999999999
          input_cost_per_token_usd: 0.0000025    # $2.50 / 1M
          cached_input_cost_per_token_usd: 0.000000625 # $0.625 / 1M
          output_cost_per_token_usd: 0.000015    # $15 / 1M
    rate_limit:
      rpm: 5
    concurrency: 2
    # Explicit temperature schedule for best-of-N=2
    temperature_schedule: [1.0, 0.2]
    thinking_budget: 32768
    use_count_tokens_in_dry_run: true
    request_timeout_s: 240  # how long you expect LLM response to take
    # max_output_tokens: 1200  # uncomment to cap completions
  gpt-oss:
    provider: openai_compat  # Use 'openai_compat' for OpenAI or compatible APIs
#    base_url: "http://localhost:4000"
    base_url: "https://bedrock-runtime.us-west-2.amazonaws.com/openai/v1"
#    model_name: gpt-oss-120b
    model_name: openai.gpt-oss-120b-1:0
    context_window: 128000
    api_key_env: OPENAI_API_KEY # Environment variable for the API key
    pricing:
      input_cost_per_token_usd: 0.00000015  # $0.15 / 1M
      output_cost_per_token_usd: 0.0000006 # $0.6 / 1M
    rate_limit:
      rpm: 2
    concurrency: 1
    request_timeout_s: 300
    # If temperature_schedule is omitted, a default is used based on 'best_of'

best_of: 3
aggregator_model_key: gemini

include_globs:
  - "**/*.c"
  - "**/*.h"
  - "**/*.cc"
  - "**/*.cpp"
  - "**/*.cxx"
  - "**/*.hh"
  - "**/*.hpp"
  - "**/*.hxx"

exclude_globs:
  - ".git/**"
  - "build/**"
  - "dist/**"
  - "out/**"
  - "target/**"
  - "vendor/**"
  - "third_party/**"

follow_symlinks: true

output_dir: llm-outputs
state_dir: .bugfinder

# Set a budget to stop scheduling when reached (finish in-flight). Null = no cap.
max_usd_total: null

# Per-model budgets (optional). Example:
# per_model_max_usd:
#   gemini: 25.0
